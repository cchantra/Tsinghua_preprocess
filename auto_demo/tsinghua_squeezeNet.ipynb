{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1f3ac7-905c-479a-af80-ac4a1b679c36",
   "metadata": {},
   "source": [
    "# SqueezeNet Auto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a6672bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.5.0\n",
      "Eager mode:  True\n",
      "GPU is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-08 20:38:25.637500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:38:25.637683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 40 deviceMemorySize: 7.76GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2021-09-08 20:38:25.637772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:38:25.637925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:38:25.638046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-09-08 20:38:25.638069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-09-08 20:38:25.638073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-09-08 20:38:25.638076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-09-08 20:38:25.638136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:38:25.638294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:38:25.638426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:0 with 6643 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import itertools\n",
    "import glob, os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from keras_tuner import HyperParameters\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner import RandomSearch,BayesianOptimization, Hyperband\n",
    "from tensorflow import keras\n",
    "\n",
    "import neptunecontrib.monitoring.kerastuner as npt_utils\n",
    "from keras_tuner import HyperParameters, Objective\n",
    "\n",
    "from keras_tuner.engine import hypermodel\n",
    "\n",
    "from model_all import SqueezeNetModel, SqueezeNet11Model,SqueezeNetSEAutoModel\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\n",
    "\n",
    "\n",
    "\n",
    "source_dir = '../TsinghuaFEDImages'\n",
    "data_dir = '../TsinghuaFED_class' # landmarks of each subject_emotion\n",
    "\n",
    "emotion_class = ['fear','neutral','surprise','anger','content','disgust','sad','happy']\n",
    "\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "nb_classes = len(emotion_class)\n",
    "ratio = 1.0\n",
    "EPOCH = 200\n",
    "\n",
    "def load_dataset(data_folder, img_dir):\n",
    "   \n",
    "\n",
    "  img_list = data_folder+'/train.csv'\n",
    "\n",
    "  f = open(img_list)\n",
    "  text = f.read()\n",
    "  lines = text.split()\n",
    "  train_img_files = []\n",
    "  train_labels = []\n",
    "  for l in lines:\n",
    "    data = l.split(',')\n",
    "    train_img_files.append(img_dir+'/'+data[0])\n",
    "    train_labels.append(int(data[1]))\n",
    "\n",
    "\n",
    "  img_train_array=[]\n",
    "  \n",
    "  img_train_array = []\n",
    "  for filename,label in zip(train_img_files,train_labels):\n",
    "      \n",
    "    image= cv2.imread( filename, cv2.COLOR_BGR2RGB)\n",
    "    image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "    image=np.array(image)\n",
    "    image = image.astype('float32')\n",
    "    image /= 255 \n",
    "    img_train_array.append(image)\n",
    "      \n",
    "\n",
    "  img_list = data_folder+'/test.csv'\n",
    "\n",
    "  f = open(img_list)\n",
    "  text = f.read()\n",
    "  lines = text.split()\n",
    "  test_img_files = []\n",
    "  test_labels = []\n",
    "  for l in lines:\n",
    "    data = l.split(',')\n",
    "    test_img_files.append(img_dir+'/'+data[0])\n",
    "    test_labels.append(int(data[1]))\n",
    "\n",
    "\n",
    "  img_test_array=[]\n",
    "\n",
    "  \n",
    "  for filename,label in zip(test_img_files,test_labels):\n",
    "\n",
    "    image= cv2.imread( filename, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "    image=np.array(image)\n",
    "    image = image.astype('float32')\n",
    "    image /= 255 \n",
    "    img_test_array.append(image)\n",
    "\n",
    "\n",
    "  return img_train_array, train_labels, img_test_array, test_labels\n",
    "    \n",
    "def transform_dataset(x_train,y_train,x_test,y_test):\n",
    "\n",
    "    \n",
    "    \n",
    "  x_test=    np.array(x_test)\n",
    "  x_test.reshape(-1, IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "  y_test= np.array(y_test)\n",
    "  x_train = np.array(x_train)\n",
    "  x_train.reshape(-1, IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "\n",
    "  y_train = np.array(y_train)\n",
    "  x_train = np.expand_dims(x_train, -1)\n",
    "  x_test = np.expand_dims(x_test, -1)\n",
    "  y_test = keras.utils.to_categorical(y_test,len(emotion_class))\n",
    "  y_train = keras.utils.to_categorical(y_train,len(emotion_class))\n",
    "  return x_train,y_train,x_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5beab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "neptune.init(project_qualified_name=' ...',api_token='....) # your credentials\n",
    "  #load data set\n",
    "x_train, y_train, x_test, y_test =load_dataset(data_dir,source_dir)\n",
    "\n",
    "x_train, y_train, x_test, y_test = transform_dataset(x_train, y_train, x_test, y_test )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748aec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/cchantra/keras-tuner/e/KER-93\n",
      "Search space summary\n",
      "Default search space size: 5\n",
      "learning_rate (Choice)\n",
      "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.0001], 'ordered': True}\n",
      "use_bypass (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "compression (Fixed)\n",
      "{'conditions': [], 'value': 1.0}\n",
      "dropout_rate (Choice)\n",
      "{'default': 0.0, 'conditions': [], 'values': [0.0, 0.5, 0.8], 'ordered': True}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'sgd'], 'ordered': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-08 20:24:38.876261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:24:38.876613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 40 deviceMemorySize: 7.76GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2021-09-08 20:24:38.876693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:24:38.877006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:24:38.877288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-09-08 20:24:38.877457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:24:38.877750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 40 deviceMemorySize: 7.76GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2021-09-08 20:24:38.877790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:24:38.878102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:24:38.878386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-09-08 20:24:38.878409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-09-08 20:24:38.878413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-09-08 20:24:38.878416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-09-08 20:24:38.878478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:24:38.878794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-08 20:24:38.879087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6643 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Squeeze random\n",
    "hp = HyperParameters()\n",
    "\n",
    "import neptune\n",
    "\n",
    "\n",
    "PROJECT = \"squeezeAutoRandom_Tsinghua\"+str(ratio)\n",
    "neptune.create_experiment(PROJECT)\n",
    "\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNetModel (classes=nb_classes, IMG_HEIGHT=IMG_HEIGHT,IMG_WIDTH=IMG_WIDTH)\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "  #build_SqueezeNet_11_fixed,\n",
    "  #build_squeezenet_auto_model,\n",
    "  #build_model,\n",
    "\n",
    "  hypermodel=mymodel,\n",
    "  hyperparameters = hp,\n",
    "  objective=Objective(\"val_accuracy\", direction=\"max\"),\n",
    "  max_trials=200,\n",
    "  executions_per_trial=1,\n",
    "  overwrite=True,\n",
    "  directory=\"mytest_tsinghua_dir\",\n",
    "  project_name=PROJECT,\n",
    "  #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "  logger = npt_utils.NeptuneLogger(),\n",
    "\n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2924ab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 00m 08s]\n",
      "val_accuracy: 0.07999999821186066\n",
      "\n",
      "Best val_accuracy So Far: 0.1371428519487381\n",
      "Total elapsed time: 00h 02m 20s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in mytest_tsinghua_dir/squeezeAutoRandom_Tsinghua1.0\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "Score: 0.1371428519487381\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: adam\n",
      "Score: 0.1257142871618271\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.09714286029338837\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: sgd\n",
      "Score: 0.09142857044935226\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: adam\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: sgd\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: adam\n",
      "Score: 0.07999999821186066\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                  max_lr=1e-3,\n",
    "                  steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                  epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "  tf.keras.callbacks.EarlyStopping('val_accuracy', patience=5),\n",
    "  #lr_finder,\n",
    "  #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "  #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models(num_models=10)\n",
    "\n",
    "\n",
    "\n",
    "tuner.results_summary()\n",
    "npt_utils.log_tuner_info(tuner)\n",
    "\n",
    "\n",
    "#best_model = tuner.get_best_models(num_models=1)[0]\n",
    "#best_model.evaluate(x_test, y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1b74ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  0 size 739528\n",
      "model  1 size 739528\n",
      "model  2 size 739528\n",
      "model  3 size 739528\n",
      "model  4 size 739528\n",
      "model  5 size 739528\n",
      "model  6 size 739528\n",
      "model  7 size 739528\n",
      "model  8 size 739528\n",
      "model  9 size 739528\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(models):\n",
    "    print('model  %d size %d'%(i,hypermodel.maybe_compute_model_size(models[i])))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e89027b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "924fd7bc",
   "metadata": {},
   "source": [
    "# Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a016ee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/cchantra/keras-tuner/e/KER-95\n",
      "Search space summary\n",
      "Default search space size: 5\n",
      "learning_rate (Choice)\n",
      "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.0001], 'ordered': True}\n",
      "use_bypass (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "compression (Fixed)\n",
      "{'conditions': [], 'value': 1.0}\n",
      "dropout_rate (Choice)\n",
      "{'default': 0.0, 'conditions': [], 'values': [0.0, 0.5, 0.8], 'ordered': True}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'sgd'], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    "#Squeeze Bayesian\n",
    "hp = HyperParameters()\n",
    "\n",
    "import neptune\n",
    "\n",
    "PROJECT = \"squeezeBayesian_Tsinghua\"+str(ratio)\n",
    "neptune.create_experiment(PROJECT)\n",
    "\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNetModel  (classes=nb_classes,IMG_HEIGHT=IMG_HEIGHT,IMG_WIDTH=IMG_WIDTH)\n",
    "\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "  #build_SqueezeNet_11_fixed,\n",
    "  #build_squeezenet_auto_model,\n",
    "  #build_model,\n",
    "\n",
    "  hypermodel=mymodel,\n",
    "  hyperparameters = hp,\n",
    "  objective=\"val_accuracy\",\n",
    "  max_trials=100,\n",
    "  executions_per_trial=1,\n",
    "  overwrite=True,\n",
    "  directory=\"mytest_tsinghua_dir\",\n",
    "  project_name= PROJECT,\n",
    "  #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "  num_initial_points=2,\n",
    "  alpha=0.0001,\n",
    "  beta=2.6,\n",
    "  seed=None,\n",
    "  tune_new_entries=True,\n",
    "  allow_new_entries=True,\n",
    "  logger = npt_utils.NeptuneLogger(),\n",
    "\n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a53a08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 00m 15s]\n",
      "val_accuracy: 0.07999999821186066\n",
      "\n",
      "Best val_accuracy So Far: 0.1542857140302658\n",
      "Total elapsed time: 00h 27m 35s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Results summary\n",
      "Results in mytest_tsinghua_dir/squeezeBayesian_Tsinghua1.0\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.1542857140302658\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.1542857140302658\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.1542857140302658\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.1371428519487381\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.1371428519487381\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.1371428519487381\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.1314285695552826\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.1314285695552826\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.1314285695552826\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.1314285695552826\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                  max_lr=1e-3,\n",
    "                  steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                  epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "  tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "  #lr_finder,\n",
    "  #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "  #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()\n",
    "npt_utils.log_tuner_info(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94058e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  0 size 739528\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "i = 0\n",
    "while i < len(models):\n",
    "    print('model  %d size %d'%(i,hypermodel.maybe_compute_model_size(models[i])))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "538d067e",
   "metadata": {},
   "source": [
    "# HyperBand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bef50530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/cchantra/keras-tuner/e/KER-96\n",
      "Search space summary\n",
      "Default search space size: 5\n",
      "learning_rate (Choice)\n",
      "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.0001], 'ordered': True}\n",
      "use_bypass (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "compression (Fixed)\n",
      "{'conditions': [], 'value': 1.0}\n",
      "dropout_rate (Choice)\n",
      "{'default': 0.0, 'conditions': [], 'values': [0.0, 0.5, 0.8], 'ordered': True}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'sgd'], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#SqueezeAuto Hyper\n",
    "hp   = HyperParameters()\n",
    "\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "import neptune\n",
    "\n",
    "PROJECT = \"squeezeHyperTsinghua\"+str(ratio)\n",
    "neptune.create_experiment(PROJECT)\n",
    "\n",
    "\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNetModel(classes=nb_classes,IMG_HEIGHT=IMG_HEIGHT,IMG_WIDTH=IMG_WIDTH)\n",
    "\n",
    "\n",
    "\n",
    "tuner = Hyperband(\n",
    "  #build_SqueezeNet_11_fixed,\n",
    "  #build_squeezenet_auto_model,\n",
    "  #build_model,\n",
    "\n",
    "  hypermodel=mymodel,\n",
    "  hyperparameters = hp,\n",
    "  objective=\"val_accuracy\",\n",
    "  #max_trials=20,\n",
    "  executions_per_trial=1,\n",
    "  overwrite=True,\n",
    "  directory=\"mytest_tsinghua_dir\",\n",
    "  project_name=PROJECT,\n",
    "  #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "  max_epochs=EPOCH,\n",
    "  logger = npt_utils.NeptuneLogger(),\n",
    "\n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a200f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24 Complete [00h 00m 05s]\n",
      "val_accuracy: 0.1257142871618271\n",
      "\n",
      "Best val_accuracy So Far: 0.1257142871618271\n",
      "Total elapsed time: 00h 02m 03s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Results summary\n",
      "Results in mytest_tsinghua_dir/squeezeHyperTsinghua1.0\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.1257142871618271\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.1257142871618271\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.11999999731779099\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.11999999731779099\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.08571428805589676\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: adam\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: adam\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.07999999821186066\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                  max_lr=1e-3,\n",
    "                  steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                  epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "  tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "  #lr_finder,\n",
    "  #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "  #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54835c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  0 size 739528\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(models):\n",
    "    print('model  %d size %d'%(i,hypermodel.maybe_compute_model_size(models[i])))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647af2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
