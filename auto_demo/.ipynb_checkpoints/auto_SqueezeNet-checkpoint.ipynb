{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.5.0\n",
      "Eager mode:  True\n",
      "WARNING:tensorflow:From <ipython-input-1-9096ef00c47e>:20: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    " \n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import glob, os\n",
    "\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptunecontrib.monitoring.kerastuner as npt_utils\n",
    "from keras_tuner import HyperParameters, Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "nb_classes = 10\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "y_test = keras.utils.to_categorical(y_test,10)\n",
    "y_train = keras.utils.to_categorical(y_train,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQUEEZENET Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper paramter squeezenet\n",
    "\n",
    " \n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner import RandomSearch, Hyperband,SklearnTuner,BayesianOptimization\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def get_axis():\n",
    "    axis = -1 if K.image_data_format() == 'channels_last' else 1\n",
    "    return axis\n",
    "\n",
    "def create_fire_module(x, nb_squeeze_filter, name, use_bypass=False):\n",
    "    \"\"\"\n",
    "    Creates a fire module\n",
    "    \n",
    "    Arguments:\n",
    "        x                 : input\n",
    "        nb_squeeze_filter : number of filters of squeeze. The filtersize of expand is 4 times of squeeze\n",
    "        use_bypass        : if True then a bypass will be added\n",
    "        name              : name of module e.g. fire123\n",
    "    \n",
    "    Returns:\n",
    "        x                 : returns a fire module\n",
    "    \"\"\"\n",
    "    \n",
    "    nb_expand_filter = 4 * nb_squeeze_filter\n",
    "    squeeze    = layers.Conv2D(nb_squeeze_filter,(1,1), activation='relu', padding='same', name='%s_squeeze'%name)(x)\n",
    "    expand_1x1 = layers.Conv2D(nb_expand_filter, (1,1), activation='relu', padding='same', name='%s_expand_1x1'%name)(squeeze)\n",
    "    expand_3x3 = layers.Conv2D(nb_expand_filter, (3,3), activation='relu', padding='same', name='%s_expand_3x3'%name)(squeeze)\n",
    "    \n",
    "    axis = get_axis()\n",
    "    x_ret =  layers.Concatenate(axis=axis, name='%s_concatenate'%name)([expand_1x1, expand_3x3])\n",
    "    \n",
    "    if use_bypass:\n",
    "        x_ret =  layers.Add(name='%s_concatenate_bypass'%name)([x_ret, x])\n",
    "        \n",
    "    return x_ret\n",
    "\n",
    "\n",
    "def output(x, nb_classes):\n",
    "    x = layers.Conv2D(nb_classes, (1,1), strides=(1,1), padding='valid', name='conv10')(x)\n",
    "    x = layers.GlobalAveragePooling2D(name='avgpool10')(x)\n",
    "    x = layers.Activation(\"softmax\", name='softmax')(x)\n",
    "    return x\n",
    "\n",
    "def build_SqueezeNet_fixed(hp):\n",
    "    \"\"\"\n",
    "    Creating a SqueezeNet of version 1.0\n",
    "    \n",
    "    Arguments:\n",
    "        input_shape  : shape of the input images e.g. (224,224,3)\n",
    "        nb_classes   : number of classes\n",
    "        use_bypass   : if true, bypass connections will be created at fire module 3, 5, 7, and 9 (default: False)\n",
    "        dropout_rate : defines the dropout rate that is accomplished after last fire module (default: None)\n",
    "        compression  : reduce the number of feature-maps (default: 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        Model        : Keras model instance\n",
    "    \"\"\"\n",
    "    use_bypass = hp.Boolean('use_bypass')\n",
    "    compression = hp.Fixed('compression',1.0)\n",
    "    \n",
    "    input_img = tf.keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "     \n",
    "    x = tf.keras.layers.Conv2D(int(96*compression), (7,7), activation='relu', strides=(2,2), padding='same', name='conv1')(input_img)\n",
    "\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool1')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(16*compression), name='fire2')\n",
    "    x = create_fire_module(x, int(16*compression), name='fire3', use_bypass=use_bypass)\n",
    "    x = create_fire_module(x, int(32*compression), name='fire4')\n",
    "    \n",
    "    x = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool4')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(32*compression), name='fire5', use_bypass=use_bypass)\n",
    "    x = create_fire_module(x, int(48*compression), name='fire6')\n",
    "    x = create_fire_module(x, int(48*compression), name='fire7', use_bypass=use_bypass)\n",
    "    x = create_fire_module(x, int(64*compression), name='fire8')\n",
    "    \n",
    "    x =  layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool8')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(64*compression), name='fire9', use_bypass=use_bypass)\n",
    "\n",
    "    dropout_rate = hp.Choice('dropout_rate',values=[0.0,0.5,0.8])\n",
    "    \n",
    "    if dropout_rate:\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "    x =  output(x, nb_classes)\n",
    "    model = keras.Model(inputs=input_img, outputs=x)\n",
    "    optimizer = hp.Choice(\"optimizer\", [\"RMSprop\" \"sgd\"])\n",
    "    model.compile(\n",
    "        optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    " \n",
    "    \n",
    "def build_SqueezeNet_11_fixed(hp):\n",
    "    \"\"\"\n",
    "    Creating a SqueezeNet of version 1.1\n",
    "    \n",
    "    2.4x less computation over SqueezeNet 1.0 implemented above.\n",
    "    \n",
    "    Arguments:\n",
    "        input_shape  : shape of the input images e.g. (224,224,3)\n",
    "        nb_classes   : number of classes\n",
    "        dropout_rate : defines the dropout rate that is accomplished after last fire module (default: None)\n",
    "        compression  : reduce the number of feature-maps\n",
    "        \n",
    "    Returns:\n",
    "        Model        : Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    use_bypass = hp.Boolean('use_bypass')\n",
    "    compression = hp.Fixed('compression',1.0)\n",
    "    \n",
    "    input_img =  layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "    \n",
    "  \n",
    "    x = layers.Conv2D(int(64*compression), (3,3), activation='relu', strides=(2,2), padding='same', name='conv1')(input_img)\n",
    "\n",
    "    x = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool1')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(16*compression), name='fire2')\n",
    "    x = create_fire_module(x, int(16*compression), name='fire3')\n",
    "    \n",
    "    x = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool3')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(32*compression), name='fire4')\n",
    "    x = create_fire_module(x, int(32*compression), name='fire5')\n",
    "    \n",
    "    x = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool5')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(48*compression), name='fire6')\n",
    "    x = create_fire_module(x, int(48*compression), name='fire7')\n",
    "    x = create_fire_module(x, int(64*compression), name='fire8')\n",
    "    x = create_fire_module(x, int(64*compression), name='fire9')\n",
    "\n",
    "    dropout_rate = hp.Choice('dropout_rate',values=[0.0,0.5,0.8])\n",
    "        \n",
    "    if dropout_rate:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Creating last conv10\n",
    "    x = output(x, nb_classes)\n",
    "\n",
    " \n",
    "    model = keras.Model(inputs=input_img, outputs=x)\n",
    "    optimizer = hp.Choice(\"optimizer\",[\"RMSprop\",\"sgd\"]) #[\"adam\", \"sgd\"])\n",
    "    model.compile(\n",
    "        optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SqueezeNet auto\n",
    "from keras_tuner import HyperModel\n",
    "\n",
    "\n",
    "class SqueezeNetAutoModel(HyperModel):\n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "\n",
    "    def build(self, hp):\n",
    "       \n",
    "          \n",
    "        compression_val = hp.Fixed('compression',1.0)\n",
    "        \n",
    "        input_img =  layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "        \n",
    "        x = layers.Conv2D(int(96*compression_val), (3,3), activation='relu', strides=(2,2), padding='same', name='conv1')(input_img)\n",
    "\n",
    "        x = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool1')(x)\n",
    "    \n",
    "         \n",
    "\n",
    "        j = 2\n",
    "        filter_size = 16\n",
    "\n",
    "        num_fire = hp.Int(\"fire_module\", 1,2, default=2)\n",
    "        use_bypass = [ hp.Boolean('use_bypass'+str(i)) for i in range(num_fire)]\n",
    "        pooling = [ hp.Choice('pooling'+str(i), [\"max\", \"avg\"]) for i in range(num_fire) ]\n",
    "        \n",
    "        print(use_bypass)\n",
    "        print(pooling)\n",
    "        for i in range(num_fire):\n",
    "         \n",
    "             \n",
    "            x = create_fire_module(x, int(filter_size*compression_val), name='fire'+str(j), )\n",
    "\n",
    "            x = create_fire_module(x, int(filter_size*compression_val), name='fire'+str(j+1),use_bypass=use_bypass[i])\n",
    "\n",
    "            #if hp.Choice(\"pooling\", [\"max\", \"avg\"]) == \"max\":\n",
    "            if pooling[i] == \"max\":\n",
    "\n",
    "                x =  layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool'+str(j+1))(x)\n",
    "            else:\n",
    "                x =  layers.AveragePooling2D(pool_size=(3,3), strides=(2,2), name='maxpool'+str(j+1))(x)\n",
    "\n",
    "\n",
    "            j = j+2\n",
    "            filter_size = filter_size+16\n",
    "            \n",
    "        num_fire2 = hp.Int('num_fire_2',0,2, default=2)\n",
    "        use_bypass2 = [ hp.Boolean('use_bypass_2'+str(i)) for i in range(num_fire2)]\n",
    "         \n",
    "        print(use_bypass2)\n",
    "         \n",
    "        for i in range(num_fire2): \n",
    "            x = create_fire_module(x, int(filter_size*compression_val), name='fire'+str(j))\n",
    "            x = create_fire_module(x, int(filter_size*compression_val), name='fire'+str(j+1),use_bypass=use_bypass2[i])\n",
    "            filter_size = filter_size+16\n",
    "            j = j+2\n",
    "\n",
    "            #x = create_fire_module(x, int(filter_size*compression_val), name='fire8')\n",
    "            #x = create_fire_module(x, int(filter_size*compression_val), name='fire9',use_bypass=hp.Boolean('use_bypass'))\n",
    "        \n",
    "        \n",
    "        \n",
    "        dropout_rate = hp.Float('dropout_rate',0.0,0.8)\n",
    "        if dropout_rate:\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "        x = output(x, self.classes)\n",
    "\n",
    "        model = keras.Model(inputs=input_img, outputs=x)\n",
    "\n",
    "        optimizer = hp.Choice(\"optimizer\", [\"RMSprop\", \"sgd\"])\n",
    "        model.compile(\n",
    "            optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SqueezeNet 11\n",
    "from keras_tuner import HyperModel\n",
    "\n",
    "\n",
    "class SqueezeNetModel(HyperModel):\n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "\n",
    "    def build(self, hp):\n",
    "       \n",
    "        use_bypass = hp.Boolean('use_bypass')\n",
    "        compression = hp.Fixed('compression',1.0)\n",
    "\n",
    "        input_img = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(int(96*compression), (7,7), activation='relu', strides=(2,2), padding='same', name='conv1')(input_img)\n",
    "\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool1')(x)\n",
    "\n",
    "        x = create_fire_module(x, int(16*compression), name='fire2')\n",
    "        x = create_fire_module(x, int(16*compression), name='fire3', use_bypass=use_bypass)\n",
    "        x = create_fire_module(x, int(32*compression), name='fire4')\n",
    "\n",
    "        x = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool4')(x)\n",
    "\n",
    "        x = create_fire_module(x, int(32*compression), name='fire5', use_bypass=use_bypass)\n",
    "        x = create_fire_module(x, int(48*compression), name='fire6')\n",
    "        x = create_fire_module(x, int(48*compression), name='fire7', use_bypass=use_bypass)\n",
    "        x = create_fire_module(x, int(64*compression), name='fire8')\n",
    "\n",
    "        x =  layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool8')(x)\n",
    "\n",
    "        x = create_fire_module(x, int(64*compression), name='fire9', use_bypass=use_bypass)\n",
    "\n",
    "        dropout_rate = hp.Choice('dropout_rate',values=[0.0,0.5,0.8])\n",
    "\n",
    "        if dropout_rate:\n",
    "            x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "        x =  output(x, self.classes)\n",
    "        model = keras.Model(inputs=input_img, outputs=x)\n",
    "        optimizer = hp.Choice(\"optimizer\", [\"RMSprop\", \"sgd\"]) # adam is not good\n",
    "        model.compile(\n",
    "            optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "        return model\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SqueezeNet 11\n",
    "from keras_tuner import HyperModel\n",
    "\n",
    "\n",
    "class SqueezeNet11Model(HyperModel):\n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "\n",
    "    def build(self, hp):\n",
    "       \n",
    "        use_bypass = hp.Boolean('use_bypass')\n",
    "        compression = hp.Fixed('compression',1.0)\n",
    "\n",
    "        input_img =  layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "\n",
    "        x = layers.Conv2D(int(64*compression), (3,3), activation='relu', strides=(2,2), padding='same', name='conv1')(input_img)\n",
    "\n",
    "        x = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool1')(x)\n",
    "\n",
    "        x = create_fire_module(x, int(16*compression), name='fire2')\n",
    "        x = create_fire_module(x, int(16*compression), name='fire3')\n",
    "\n",
    "        x = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool3')(x)\n",
    "\n",
    "        x = create_fire_module(x, int(32*compression), name='fire4')\n",
    "        x = create_fire_module(x, int(32*compression), name='fire5')\n",
    "\n",
    "        x = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool5')(x)\n",
    "\n",
    "        x = create_fire_module(x, int(48*compression), name='fire6')\n",
    "        x = create_fire_module(x, int(48*compression), name='fire7')\n",
    "        x = create_fire_module(x, int(64*compression), name='fire8')\n",
    "        x = create_fire_module(x, int(64*compression), name='fire9')\n",
    "\n",
    "        dropout_rate = hp.Choice('dropout_rate',values=[0.1,0.5,0.8])\n",
    "\n",
    "        if dropout_rate:\n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "        # Creating last conv10\n",
    "        x = output(x, self.classes)\n",
    "        model = keras.Model(inputs=input_img, outputs=x)\n",
    "        optimizer = hp.Choice(\"optimizer\",     [\"sgd\",\"RMSprop\"])\n",
    "        model.compile(\n",
    "            optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gist.github.com/jeremyjordan/ac0229abd4b2b7000aca1643e88e0f02\n",
    "\n",
    "#learning rate callbacks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    \n",
    "    '''\n",
    "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
    "    \n",
    "    # Usage\n",
    "        ```python\n",
    "            lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
    "            \n",
    "            lr_finder.plot_loss()\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
    "        \n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: https://arxiv.org/abs/1506.01186\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-3, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(cchantra/keras-tuner)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neptune\n",
    "neptune.init(project_qualified_name='cchantra/keras-tuner',api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0M2NkYzQyZC01M2MzLTRhYjQtOTQ5Ny05NGY0NTU5MmU2NjUifQ==') # your credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 06m 20s]\n",
      "val_accuracy: 0.5565000176429749\n",
      "\n",
      "Best val_accuracy So Far: 0.5920000076293945\n",
      "Total elapsed time: 04h 55m 05s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[False]\n",
      "['avg']\n",
      "[True]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Results summary\n",
      "Results in mytest_dir/squeezeAutoRandom\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: False\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: max\n",
      "num_fire_2: 1\n",
      "use_bypass_20: True\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.4827245126580261\n",
      "optimizer: sgd\n",
      "Score: 0.5920000076293945\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: False\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: max\n",
      "num_fire_2: 1\n",
      "use_bypass_20: False\n",
      "use_bypass_21: True\n",
      "dropout_rate: 0.4825158750491049\n",
      "optimizer: sgd\n",
      "Score: 0.5910000205039978\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: False\n",
      "use_bypass1: False\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 1\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.3653306947027153\n",
      "optimizer: sgd\n",
      "Score: 0.5814999938011169\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: max\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.41135551532828346\n",
      "optimizer: sgd\n",
      "Score: 0.5724999904632568\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: False\n",
      "use_bypass1: False\n",
      "pooling0: avg\n",
      "pooling1: max\n",
      "num_fire_2: 1\n",
      "use_bypass_20: False\n",
      "use_bypass_21: True\n",
      "dropout_rate: 0.7773545024206198\n",
      "optimizer: sgd\n",
      "Score: 0.5695000290870667\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: False\n",
      "use_bypass1: False\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 1\n",
      "use_bypass_20: False\n",
      "use_bypass_21: True\n",
      "dropout_rate: 0.42290355892630976\n",
      "optimizer: RMSprop\n",
      "Score: 0.5674999952316284\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: False\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: True\n",
      "dropout_rate: 0.25219939221519727\n",
      "optimizer: sgd\n",
      "Score: 0.5674999952316284\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 1\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.4793930302619412\n",
      "optimizer: sgd\n",
      "Score: 0.5649999976158142\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: max\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: True\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.2270644377789089\n",
      "optimizer: sgd\n",
      "Score: 0.5590000152587891\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: max\n",
      "num_fire_2: 2\n",
      "use_bypass_20: True\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.6702292767958894\n",
      "optimizer: sgd\n",
      "Score: 0.5580000281333923\n"
     ]
    }
   ],
   "source": [
    "#SqueezeAuto random\n",
    "hp = HyperParameters()\n",
    "\n",
    "import neptune\n",
    "\n",
    "\n",
    "neptune.create_experiment(\"squeezeAutoRandom\")\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNetAutoModel(classes=nb_classes)\n",
    "\n",
    " \n",
    "tuner = RandomSearch(\n",
    "    #build_SqueezeNet_11_fixed,\n",
    "    #build_squeezenet_auto_model,\n",
    "    #build_model,\n",
    "    \n",
    "    hypermodel=mymodel,\n",
    "    hyperparameters = hp,\n",
    "    objective=Objective(\"val_accuracy\", direction=\"max\"),\n",
    "    max_trials=100,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"mytest_dir\",\n",
    "    project_name=\"squeezeAutoRandom\",\n",
    "    #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    logger = npt_utils.NeptuneLogger(),\n",
    "     \n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "ratio = 0.2\n",
    "EPOCH = 200\n",
    "batch_size = 16\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                    max_lr=1e-3,\n",
    "                    steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                    epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "    #lr_finder,\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()\n",
    "npt_utils.log_tuner_info(tuner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 254 Complete [00h 02m 46s]\n",
      "val_accuracy: 0.453000009059906\n",
      "\n",
      "Best val_accuracy So Far: 0.5789999961853027\n",
      "Total elapsed time: 04h 04m 52s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[True]\n",
      "['avg']\n",
      "[False]\n",
      "Results summary\n",
      "Results in mytest_dir/squeezeAutoHyper\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: False\n",
      "pooling0: avg\n",
      "pooling1: max\n",
      "num_fire_2: 1\n",
      "use_bypass_20: False\n",
      "use_bypass_21: True\n",
      "dropout_rate: 0.43558991227985394\n",
      "optimizer: RMSprop\n",
      "tuner/epochs: 67\n",
      "tuner/initial_epoch: 23\n",
      "tuner/bracket: 4\n",
      "tuner/round: 3\n",
      "tuner/trial_id: d30c412826257ee58a35fdc52b6743ee\n",
      "Score: 0.5789999961853027\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: False\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.4449650938754281\n",
      "optimizer: sgd\n",
      "tuner/epochs: 200\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.5674999952316284\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "compression: 1.0\n",
      "fire_module: 2\n",
      "use_bypass0: True\n",
      "use_bypass1: False\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 1\n",
      "use_bypass_20: True\n",
      "use_bypass_21: True\n",
      "dropout_rate: 0.7615247463440488\n",
      "optimizer: sgd\n",
      "tuner/epochs: 67\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.5649999976158142\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "compression: 1.0\n",
      "fire_module: 2\n",
      "use_bypass0: True\n",
      "use_bypass1: False\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 1\n",
      "use_bypass_20: True\n",
      "use_bypass_21: True\n",
      "dropout_rate: 0.7615247463440488\n",
      "optimizer: sgd\n",
      "tuner/epochs: 200\n",
      "tuner/initial_epoch: 67\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 768f1c13bd26f78507e405ccd373384e\n",
      "Score: 0.5619999766349792\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 2\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: max\n",
      "pooling1: avg\n",
      "num_fire_2: 1\n",
      "use_bypass_20: True\n",
      "use_bypass_21: True\n",
      "dropout_rate: 0.20399315413266753\n",
      "optimizer: sgd\n",
      "tuner/epochs: 67\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.5584999918937683\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 2\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 1\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.20760396382135193\n",
      "optimizer: RMSprop\n",
      "tuner/epochs: 200\n",
      "tuner/initial_epoch: 67\n",
      "tuner/bracket: 3\n",
      "tuner/round: 3\n",
      "tuner/trial_id: c12dc021f644f16927787f6e5ed89d19\n",
      "Score: 0.5565000176429749\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: False\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 1\n",
      "use_bypass_20: False\n",
      "use_bypass_21: True\n",
      "dropout_rate: 0.47493749923613315\n",
      "optimizer: RMSprop\n",
      "tuner/epochs: 67\n",
      "tuner/initial_epoch: 23\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: af8fb4f67c6ac183d9f3e378bea09b16\n",
      "Score: 0.5544999837875366\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 2\n",
      "use_bypass0: True\n",
      "use_bypass1: False\n",
      "pooling0: max\n",
      "pooling1: max\n",
      "num_fire_2: 1\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.16337967451604732\n",
      "optimizer: RMSprop\n",
      "tuner/epochs: 200\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.5540000200271606\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: max\n",
      "num_fire_2: 1\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.19971369280874668\n",
      "optimizer: RMSprop\n",
      "tuner/epochs: 200\n",
      "tuner/initial_epoch: 67\n",
      "tuner/bracket: 4\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 6250d573f8e789e20dd0e7c4e749e69b\n",
      "Score: 0.5485000014305115\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 2\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 1\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.20760396382135193\n",
      "optimizer: RMSprop\n",
      "tuner/epochs: 67\n",
      "tuner/initial_epoch: 23\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 3648d31790dd3d7a11839a0772988207\n",
      "Score: 0.5485000014305115\n"
     ]
    }
   ],
   "source": [
    "#SqueezeAuto Hyper\n",
    "hp = HyperParameters()\n",
    "\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "import neptune\n",
    "\n",
    "\n",
    "neptune.create_experiment(\"squeezeAutoHyper\")\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNetAutoModel(classes=nb_classes)\n",
    "\n",
    "ratio = 0.2\n",
    "EPOCH = 200\n",
    "batch_size = 16\n",
    "tuner = Hyperband(\n",
    "    #build_SqueezeNet_11_fixed,\n",
    "    #build_squeezenet_auto_model,\n",
    "    #build_model,\n",
    "    \n",
    "    hypermodel=mymodel,\n",
    "    hyperparameters = hp,\n",
    "    objective=\"val_accuracy\",\n",
    "    #max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"mytest_dir\",\n",
    "    project_name=\"squeezeAutoHyper\",\n",
    "    #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    max_epochs=EPOCH,\n",
    "    logger = npt_utils.NeptuneLogger(),\n",
    "     \n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                    max_lr=1e-3,\n",
    "                    steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                    epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "    #lr_finder,\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 03m 25s]\n",
      "val_accuracy: 0.5385000109672546\n",
      "\n",
      "Best val_accuracy So Far: 0.5565000176429749\n",
      "Total elapsed time: 01h 07m 08s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[True]\n",
      "['avg']\n",
      "[False, False]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.rho\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Results summary\n",
      "Results in mytest_dir/squeezeAutoBayesian\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5565000176429749\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5550000071525574\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5540000200271606\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5509999990463257\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: max\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.546999990940094\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: max\n",
      "pooling1: max\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5460000038146973\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5460000038146973\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: avg\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5460000038146973\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: True\n",
      "use_bypass1: True\n",
      "pooling0: max\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5450000166893005\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "compression: 1.0\n",
      "fire_module: 1\n",
      "use_bypass0: False\n",
      "use_bypass1: True\n",
      "pooling0: max\n",
      "pooling1: avg\n",
      "num_fire_2: 2\n",
      "use_bypass_20: False\n",
      "use_bypass_21: False\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5450000166893005\n"
     ]
    }
   ],
   "source": [
    "#SqueezeAuto Bayes\n",
    "hp = HyperParameters()\n",
    "\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "import neptune\n",
    "\n",
    "\n",
    "neptune.create_experiment(\"squeezeAutoBaysian\")\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNetAutoModel(classes=nb_classes)\n",
    "\n",
    " \n",
    "    \n",
    "ratio = 0.2\n",
    "EPOCH = 200\n",
    "batch_size = 16\n",
    "tuner = BayesianOptimization(\n",
    "    #build_SqueezeNet_11_fixed,\n",
    "    #build_squeezenet_auto_model,\n",
    "    #build_model,\n",
    "    \n",
    "    hypermodel=mymodel,\n",
    "    hyperparameters = hp,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=100,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"mytest_dir\",\n",
    "    project_name=\"squeezeAutoBayesian\",\n",
    "    #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    num_initial_points=2,\n",
    "    alpha=0.0001,\n",
    "    beta=2.6,\n",
    "    seed=None,\n",
    "    tune_new_entries=True,\n",
    "    allow_new_entries=True,\n",
    "    logger = npt_utils.NeptuneLogger(),\n",
    "     \n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                    max_lr=1e-3,\n",
    "                    steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                    epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "    #lr_finder,\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 Complete [00h 07m 19s]\n",
      "val_accuracy: 0.5065000057220459\n",
      "\n",
      "Best val_accuracy So Far: 0.5379999876022339\n",
      "Total elapsed time: 01h 22m 10s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.rho\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Results summary\n",
      "Results in mytest_dir/squeeze11Random\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: sgd\n",
      "Score: 0.5379999876022339\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: sgd\n",
      "Score: 0.5335000157356262\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: sgd\n",
      "Score: 0.5304999947547913\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: sgd\n",
      "Score: 0.5295000076293945\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: sgd\n",
      "Score: 0.5254999995231628\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: sgd\n",
      "Score: 0.5205000042915344\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: RMSprop\n",
      "Score: 0.5149999856948853\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.5065000057220459\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: RMSprop\n",
      "Score: 0.5049999952316284\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.4984999895095825\n"
     ]
    },
    {
     "ename": "NeptuneException",
     "evalue": "\n\u001b[95m\n----ClientHttpError-----------------------------------------------------------------------\n\u001b[0m\nNeptune server returned status \u001b[91m400\u001b[0m.\n\nServer response was:\n\u001b[91m{\"title\":\"Length of stream does not match given range\",\"code\":400,\"errorType\":\"BAD_REQUEST\"}\u001b[0m\n\nVerify the correctness of your call or contact Neptune support.\n\n\u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting-started/getting-help\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/new/internal/backends/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/new/internal/backends/hosted_file_operations.py\u001b[0m in \u001b[0;36mupload_raw_data\u001b[0;34m(http_client, url, data, path_params, query_params, headers)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNeptuneStorageLimitException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error:  for url: https://app.neptune.ai/api/leaderboard/v1/attributes/upload?experimentId=0ed151da-ce96-4ee7-9129-b59baa8e4a3b&attribute=artifacts%2Fsqueeze11Random%2Ftrial_a7203d5012cf9f216452d8f06c2f85b4%2Ftrial.json&ext=",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mClientHttpError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/internal/api_clients/hosted_api_clients/hosted_alpha_leaderboard_api_client.py\u001b[0m in \u001b[0;36m_execute_upload_operation\u001b[0;34m(self, experiment, upload_operation)\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupload_operation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m                     ext=upload_operation.ext)\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupload_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_operation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUploadFileContent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/new/internal/backends/hosted_file_operations.py\u001b[0m in \u001b[0;36mupload_file_attribute\u001b[0;34m(swagger_client, run_uuid, attribute, source, ext)\u001b[0m\n\u001b[1;32m     63\u001b[0m                          \u001b[0;34m\"attribute\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                          \u001b[0;34m\"ext\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                      })\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/new/internal/backends/hosted_file_operations.py\u001b[0m in \u001b[0;36m_upload_loop\u001b[0;34m(file_chunk_stream, response_handler, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_chunk_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_loop_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_chunk_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mresponse_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/new/internal/backends/hosted_file_operations.py\u001b[0m in \u001b[0;36m_upload_loop_chunk\u001b[0;34m(chunk, file_chunk_stream, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X-File-Permissions\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_chunk_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermissions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mupload_raw_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/new/internal/backends/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mClientHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientHttpError\u001b[0m: \n\u001b[95m\n----ClientHttpError-----------------------------------------------------------------------\n\u001b[0m\nNeptune server returned status \u001b[91m400\u001b[0m.\n\nServer response was:\n\u001b[91m{\"title\":\"Length of stream does not match given range\",\"code\":400,\"errorType\":\"BAD_REQUEST\"}\u001b[0m\n\nVerify the correctness of your call or contact Neptune support.\n\n\u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting-started/getting-help\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNeptuneException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3d2c49fd9ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(num_models=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mnpt_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_tuner_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptunecontrib/monitoring/kerastuner.py\u001b[0m in \u001b[0;36mlog_tuner_info\u001b[0;34m(tuner, experiment, log_project_dir)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlog_project_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_parameters'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/__init__.py\u001b[0m in \u001b[0;36mlog_artifact\u001b[0;34m(artifact, destination)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mAlias\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mneptune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \"\"\"\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/experiments.py\u001b[0m in \u001b[0;36mlog_artifact\u001b[0;34m(self, artifact, destination)\u001b[0m\n\u001b[1;32m    621\u001b[0m                 \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/wrong_prediction_1.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images/my_image_1.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \"\"\"\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/internal/api_clients/hosted_api_clients/hosted_alpha_leaderboard_api_client.py\u001b[0m in \u001b[0;36mlog_artifact\u001b[0;34m(self, experiment, artifact, destination)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_destination\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_dir_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_destination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/internal/api_clients/hosted_api_clients/hosted_alpha_leaderboard_api_client.py\u001b[0m in \u001b[0;36mlog_artifact\u001b[0;34m(self, experiment, artifact, destination)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Artifact must be a local path or an IO object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_upload_operations_with_400_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/internal/api_clients/hosted_api_clients/hosted_alpha_leaderboard_api_client.py\u001b[0m in \u001b[0;36m_execute_upload_operations_with_400_retry\u001b[0;34m(self, experiment, upload_operation)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_upload_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupload_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mClientHttpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"Length of stream does not match given range\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/neptune/internal/api_clients/hosted_api_clients/hosted_alpha_leaderboard_api_client.py\u001b[0m in \u001b[0;36m_execute_upload_operation\u001b[0;34m(self, experiment, upload_operation)\u001b[0m\n\u001b[1;32m    593\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNeptuneException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Upload operation in neither File or FileSet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0malpha_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeptuneException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNeptuneException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNeptuneException\u001b[0m: \n\u001b[95m\n----ClientHttpError-----------------------------------------------------------------------\n\u001b[0m\nNeptune server returned status \u001b[91m400\u001b[0m.\n\nServer response was:\n\u001b[91m{\"title\":\"Length of stream does not match given range\",\"code\":400,\"errorType\":\"BAD_REQUEST\"}\u001b[0m\n\nVerify the correctness of your call or contact Neptune support.\n\n\u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting-started/getting-help\n"
     ]
    }
   ],
   "source": [
    "#SqueezeNet 11 Random\n",
    "\n",
    "from keras_tuner import HyperParameters\n",
    "hp = HyperParameters()\n",
    "\n",
    "import neptune\n",
    "\n",
    "\n",
    "neptune.create_experiment(\"squeeze11Random\")\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNet11Model(classes=nb_classes)\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    #build_SqueezeNet_11_fixed,\n",
    "    #build_squeezenet_auto_model,\n",
    "    #build_model,\n",
    "    \n",
    "    hypermodel=mymodel,\n",
    "    hyperparameters = hp,\n",
    "    objective=Objective(\"val_accuracy\", direction=\"max\"),\n",
    "    max_trials=100,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"mytest_dir\",\n",
    "    project_name=\"squeeze11Random\",\n",
    "    #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    logger = npt_utils.NeptuneLogger(),\n",
    "     \n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "ratio = 0.2\n",
    "EPOCH = 200\n",
    "batch_size = 16\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                    max_lr=1e-3,\n",
    "                    steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                    epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "    #lr_finder,\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()\n",
    "npt_utils.log_tuner_info(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24 Complete [00h 00m 21s]\n",
      "val_accuracy: 0.18299999833106995\n",
      "\n",
      "Best val_accuracy So Far: 0.265500009059906\n",
      "Total elapsed time: 00h 08m 11s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Results summary\n",
      "Results in mytest_dir/squeeze110.2\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: RMSprop\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.265500009059906\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: RMSprop\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.26249998807907104\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: RMSprop\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.2515000104904175\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: RMSprop\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.24799999594688416\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.2460000067949295\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: RMSprop\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.24199999868869781\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.23250000178813934\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.22699999809265137\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.21899999678134918\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: RMSprop\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.21699999272823334\n"
     ]
    }
   ],
   "source": [
    "#Squeeze11 Hyper\n",
    "hp = HyperParameters()\n",
    "\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "import neptune\n",
    "\n",
    "\n",
    "ratio = 0.2\n",
    "PROJECT = \"squeeze11\"+str(ratio)\n",
    "neptune.create_experiment(PROJECT)\n",
    " \n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNet11Model(classes=nb_classes)\n",
    "\n",
    "ratio = 0.2\n",
    "EPOCH = 200\n",
    "batch_size = 16\n",
    "tuner = Hyperband(\n",
    "    #build_SqueezeNet_11_fixed,\n",
    "    #build_squeezenet_auto_model,\n",
    "    #build_model,\n",
    "    \n",
    "    hypermodel=mymodel,\n",
    "    hyperparameters = hp,\n",
    "    objective=\"val_accuracy\",\n",
    "    #max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"mytest_dir\",\n",
    "    project_name=PROJECT,\n",
    "    #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    max_epochs=EPOCH,\n",
    "    logger = npt_utils.NeptuneLogger(),\n",
    "     \n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                    max_lr=1e-3,\n",
    "                    steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                    epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "    #lr_finder,\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 200 Complete [00h 04m 14s]\n",
      "val_accuracy: 0.5170000195503235\n",
      "\n",
      "Best val_accuracy So Far: 0.5559999942779541\n",
      "Total elapsed time: 18h 12m 21s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.rho\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Results summary\n",
      "Results in mytest_dir/squeeze11Bayesian\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.5559999942779541\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.550000011920929\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.546999990940094\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.5450000166893005\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.5440000295639038\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.5435000061988831\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.5419999957084656\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.5419999957084656\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.5389999747276306\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.5389999747276306\n"
     ]
    }
   ],
   "source": [
    "#SqueezeAuto Bayes\n",
    "hp = HyperParameters()\n",
    "\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "import neptune\n",
    "\n",
    "\n",
    "neptune.create_experiment(\"squeeze11Baysian\")\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNet11Model(classes=nb_classes)\n",
    "\n",
    " \n",
    "    \n",
    "ratio = 0.2\n",
    "EPOCH = 200\n",
    "batch_size = 16\n",
    "tuner = BayesianOptimization(\n",
    "    #build_SqueezeNet_11_fixed,\n",
    "    #build_squeezenet_auto_model,\n",
    "    #build_model,\n",
    "    \n",
    "    hypermodel=mymodel,\n",
    "    hyperparameters = hp,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=200,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"mytest_dir\",\n",
    "    project_name=\"squeeze11Bayesian\",\n",
    "    #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    num_initial_points=2,\n",
    "    alpha=0.0001,\n",
    "    beta=2.6,\n",
    "    seed=None,\n",
    "    tune_new_entries=True,\n",
    "    allow_new_entries=True,\n",
    "    logger = npt_utils.NeptuneLogger(),\n",
    "     \n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                    max_lr=1e-3,\n",
    "                    steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                    epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "    #lr_finder,\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "  strategy = tf.distribute.MirroredStrategy()\n",
    "else:  # Use the Default Strategy\n",
    "  strategy = tf.distribute.get_strategy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 Complete [00h 01m 11s]\n",
      "val_accuracy: 0.10849999636411667\n",
      "\n",
      "Best val_accuracy So Far: 0.5090000033378601\n",
      "Total elapsed time: 00h 48m 43s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in mytest_dir/squeezeRandom0.2\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: sgd\n",
      "Score: 0.5090000033378601\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.5084999799728394\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: sgd\n",
      "Score: 0.5080000162124634\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.5009999871253967\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.4945000112056732\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.4724999964237213\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: sgd\n",
      "Score: 0.4690000116825104\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: RMSprop\n",
      "Score: 0.46700000762939453\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.43950000405311584\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: RMSprop\n",
      "Score: 0.10849999636411667\n"
     ]
    }
   ],
   "source": [
    "#SqueezeNet Random\n",
    " \n",
    "from keras_tuner import HyperParameters\n",
    "hp = HyperParameters()\n",
    "import neptune\n",
    "\n",
    "\n",
    "ratio = 0.2\n",
    "PROJECT = \"squeezeRandom\"+str(ratio)\n",
    "neptune.create_experiment(PROJECT)\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNetModel(classes=nb_classes)\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    #build_SqueezeNet_11_fixed,\n",
    "    #build_squeezenet_auto_model,\n",
    "    #build_model,\n",
    "    \n",
    "    hypermodel=mymodel,\n",
    "    hyperparameters = hp,\n",
    "    objective=Objective(\"val_accuracy\", direction=\"max\"),\n",
    "    max_trials=100,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"mytest_dir\",\n",
    "    project_name=PROJECT,\n",
    "     \n",
    "    #distribution_strategy=strategy,\n",
    "    logger = npt_utils.NeptuneLogger(),\n",
    "     \n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    " \n",
    "EPOCH = 200\n",
    "batch_size = 16\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                    max_lr=1e-3,\n",
    "                    steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                    epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "    #lr_finder,\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()\n",
    "\n",
    "npt_utils.log_tuner_info(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy at 0x7f6fcff485f8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Squeeze11 Hyper\n",
    "hp = HyperParameters()\n",
    "\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "import neptune\n",
    "\n",
    "\n",
    "ratio = 0.2\n",
    "PROJECT = \"squeeze11\"+str(ratio)\n",
    "neptune.create_experiment(PROJECT)\n",
    " \n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNetModel(classes=nb_classes)\n",
    "\n",
    "ratio = 0.2\n",
    "EPOCH = 200\n",
    "batch_size = 16\n",
    "tuner = Hyperband(\n",
    "    #build_SqueezeNet_11_fixed,\n",
    "    #build_squeezenet_auto_model,\n",
    "    #build_model,\n",
    "    \n",
    "    hypermodel=mymodel,\n",
    "    hyperparameters = hp,\n",
    "    objective=\"val_accuracy\",\n",
    "    #max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"mytest_dir\",\n",
    "    project_name=PROJECT,\n",
    "    #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    max_epochs=EPOCH,\n",
    "    logger = npt_utils.NeptuneLogger(),\n",
    "     \n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                    max_lr=1e-3,\n",
    "                    steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                    epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "    #lr_finder,\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/cchantra/keras-tuner/e/KER-39\n",
      "https://app.neptune.ai/cchantra/keras-tuner/e/KER-40\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'max_trials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9e98ec01bc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m#distribution_strategy=tf.distribute.MirroredStrategy(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpt_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeptuneLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras_tuner/tuners/hyperband.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hypermodel, objective, max_epochs, factor, hyperband_iterations, seed, hyperparameters, tune_new_entries, allow_new_entries, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m         )\n\u001b[1;32m    361\u001b[0m         super(Hyperband, self).__init__(\n\u001b[0;32m--> 362\u001b[0;31m             \u001b[0moracle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypermodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         )\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras_tuner/engine/multi_execution_tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, oracle, hypermodel, executions_per_trial, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moracle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypermodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutions_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiExecutionTuner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypermodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'max_trials'"
     ]
    }
   ],
   "source": [
    "#SqueezeNet Hyper\n",
    "hp = HyperParameters()\n",
    "\n",
    "import neptune\n",
    "\n",
    "\n",
    "ratio = 0.2\n",
    "PROJECT = \"squeezeHyper\"+str(ratio)\n",
    "neptune.create_experiment(PROJECT)\n",
    "\n",
    "neptune.create_experiment(PROJECT)\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNetModel(classes=nb_classes)\n",
    "\n",
    "EPOCH = 200\n",
    "batch_size = 16\n",
    "tuner = Hyperband(\n",
    "    #build_SqueezeNet_11_fixed,\n",
    "    #build_squeezenet_auto_model,\n",
    "    #build_model,\n",
    "    \n",
    "    hypermodel=mymodel,\n",
    "    max_trials=100,\n",
    "    hyperparameters = hp,\n",
    "    objective=Objective(\"val_accuracy\", direction=\"max\"),\n",
    "    #max_trials=100,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"mytest_dir\",\n",
    "    project_name=PROJECT,\n",
    "    #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    max_epochs=EPOCH,\n",
    "    logger = npt_utils.NeptuneLogger(),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                    max_lr=1e-3,\n",
    "                    steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                    epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "    #lr_finder,\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()\n",
    "npt_utils.log_tuner_info(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 02m 26s]\n",
      "val_accuracy: 0.47699999809265137\n",
      "\n",
      "Best val_accuracy So Far: 0.5199999809265137\n",
      "Total elapsed time: 05h 48m 29s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Results summary\n",
      "Results in mytest_dir/squeezeBayesian0.2\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5199999809265137\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.515999972820282\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.515500009059906\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.515500009059906\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5145000219345093\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5134999752044678\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5134999752044678\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5120000243186951\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5109999775886536\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.0\n",
      "optimizer: sgd\n",
      "Score: 0.5099999904632568\n"
     ]
    }
   ],
   "source": [
    "#SqueezeAuto Bayes\n",
    "hp = HyperParameters()\n",
    "\n",
    "\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "import neptune\n",
    "\n",
    "ratio = 0.2\n",
    "PROJECT = \"squeezeBayesian\"+str(ratio)\n",
    "neptune.create_experiment(PROJECT)\n",
    "\n",
    " \n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNetModel(classes=nb_classes)\n",
    "\n",
    " \n",
    "    \n",
    "EPOCH = 200\n",
    "batch_size = 16\n",
    "tuner = BayesianOptimization(\n",
    "    #build_SqueezeNet_11_fixed,\n",
    "    #build_squeezenet_auto_model,\n",
    "    #build_model,\n",
    "    \n",
    "    hypermodel=mymodel,\n",
    "    hyperparameters = hp,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=100,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"mytest_dir\",\n",
    "    project_name=PROJECT,\n",
    "    #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    num_initial_points=2,\n",
    "    alpha=0.0001,\n",
    "    beta=2.6,\n",
    "    seed=None,\n",
    "    tune_new_entries=True,\n",
    "    allow_new_entries=True,\n",
    "    logger = npt_utils.NeptuneLogger(),\n",
    "     \n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                    max_lr=1e-3,\n",
    "                    steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                    epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "    #lr_finder,\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected error in ping thread.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 384, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 2, in raise_from\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/http/client.py\", line 1321, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/http/client.py\", line 296, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/http/client.py\", line 257, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\", line 299, in recv_into\n",
      "    raise SocketError(str(e))\n",
      "OSError: (104, 'ECONNRESET')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\", line 367, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/urllib3/packages/six.py\", line 685, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 384, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 2, in raise_from\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/http/client.py\", line 1321, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/http/client.py\", line 296, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/http/client.py\", line 257, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\", line 299, in recv_into\n",
      "    raise SocketError(str(e))\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', OSError(\"(104, 'ECONNRESET')\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/bravado/http_future.py\", line 124, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/bravado/http_future.py\", line 291, in _get_incoming_response\n",
      "    inner_response = self.future.result(timeout=timeout)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/bravado/requests_client.py\", line 270, in result\n",
      "    **settings\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 498, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', OSError(\"(104, 'ECONNRESET')\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/neptune/internal/threads/ping_thread.py\", line 37, in run\n",
      "    self.__backend.ping_experiment(self.__experiment)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/neptune/internal/api_clients/hosted_api_clients/hosted_alpha_leaderboard_api_client.py\", line 393, in ping_experiment\n",
      "    self.leaderboard_swagger_client.api.ping(experimentId=str(experiment.internal_id)).response().result\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/bravado/http_future.py\", line 239, in response\n",
      "    six.reraise(*sys.exc_info())\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/six.py\", line 703, in reraise\n",
      "    raise value\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/bravado/http_future.py\", line 197, in response\n",
      "    incoming_response = self._get_incoming_response(timeout)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/bravado/http_future.py\", line 126, in wrapper\n",
      "    self.future._raise_connection_error(exception)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/bravado/http_future.py\", line 91, in _raise_connection_error\n",
      "    self._raise_error(BravadoConnectionError, 'ConnectionError', exception)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/bravado/http_future.py\", line 82, in _raise_error\n",
      "    sys.exc_info()[2],\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/six.py\", line 702, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/bravado/http_future.py\", line 124, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/bravado/http_future.py\", line 291, in _get_incoming_response\n",
      "    inner_response = self.future.result(timeout=timeout)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/bravado/requests_client.py\", line 270, in result\n",
      "    **settings\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/chantana/anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 498, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "bravado.http_future.RequestsFutureAdapterConnectionError\n"
     ]
    }
   ],
   "source": [
    "class MyTuner(kerastuner.tuners.BayesianOptimization):\n",
    "  def run_trial(self, trial, *args, **kwargs):\n",
    "    # You can add additional HyperParameters for preprocessing and custom training loops\n",
    "    # via overriding `run_trial`\n",
    "    kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 32, 256, step=32)\n",
    "    kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 30)\n",
    "    super(MyTuner, self).run_trial(trial, *args, **kwargs)\n",
    "\n",
    "# Uses same arguments as the BayesianOptimization Tuner.\n",
    "tuner = MyTuner(...)\n",
    "# Don't pass epochs or batch_size here, let the Tuner tune them.\n",
    "tuner.search(...)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import kerastuner\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "\n",
    "class CVTuner(kerastuner.engine.tuner.Tuner):\n",
    "  def run_trial(self, trial, x, y, batch_size=32, epochs=1):\n",
    "    cv = model_selection.KFold(5)\n",
    "    val_losses = []\n",
    "    for train_indices, test_indices in cv.split(x):\n",
    "      x_train, x_test = x[train_indices], x[test_indices]\n",
    "      y_train, y_test = y[train_indices], y[test_indices]\n",
    "      model = self.hypermodel.build(trial.hyperparameters)\n",
    "      model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "      val_losses.append(model.evaluate(x_test, y_test))\n",
    "    self.oracle.update_trial(trial.trial_id, {'val_loss': np.mean(val_losses)})\n",
    "    self.save_model(trial.trial_id, model)\n",
    "\n",
    "tuner = CVTuner(\n",
    "  hypermodel=my_build_model,\n",
    "  oracle=kerastuner.oracles.BayesianOptimization(\n",
    "    objective='val_loss',\n",
    "    max_trials=40))\n",
    "\n",
    "x, y = ...  # NumPy data\n",
    "tuner.search(x, y, batch_size=64, epochs=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tsinghua dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_dir = '../TsinghuaFEDImages'\n",
    "data_dir = '../TsinghuaFEDClass' # landmarks of each subject_emotion\n",
    "\n",
    "emotion_class = ['fear','neutral','surprise','anger','content','disgust','sad','happy']\n",
    "\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "nb_classes = len(emotion_class)\n",
    "\n",
    "def load_dataset(data_folder, img_dir):\n",
    "   \n",
    "\n",
    "    img_list = data_folder+'/train.csv'\n",
    "\n",
    "    f = open(img_list)\n",
    "    text = f.read()\n",
    "    lines = text.split()\n",
    "    train_img_files = []\n",
    "    train_labels = []\n",
    "    for l in lines:\n",
    "      data = l.split(',')\n",
    "      train_img_files.append(img_dir+'/'+data[0])\n",
    "      train_labels.append(int(data[1]))\n",
    "\n",
    "\n",
    "    img_train_array=[]\n",
    "   \n",
    "    img_train_array = []\n",
    "    for filename,label in zip(train_img_files,train_labels):\n",
    "       \n",
    "      image= cv2.imread( filename, cv2.COLOR_BGR2RGB)\n",
    "      image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "      image=np.array(image)\n",
    "      image = image.astype('float32')\n",
    "      image /= 255 \n",
    "      img_train_array.append(image)\n",
    "       \n",
    "\n",
    "    img_list = data_folder+'/test.csv'\n",
    "\n",
    "    f = open(img_list)\n",
    "    text = f.read()\n",
    "    lines = text.split()\n",
    "    test_img_files = []\n",
    "    test_labels = []\n",
    "    for l in lines:\n",
    "      data = l.split(',')\n",
    "      test_img_files.append(img_dir+'/'+data[0])\n",
    "      test_labels.append(int(data[1]))\n",
    "\n",
    "\n",
    "    img_test_array=[]\n",
    "  \n",
    "   \n",
    "    for filename,label in zip(test_img_files,test_labels):\n",
    "\n",
    "      image= cv2.imread( filename, cv2.COLOR_BGR2RGB)\n",
    "      \n",
    "      image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "      image=np.array(image)\n",
    "      image = image.astype('float32')\n",
    "      image /= 255 \n",
    "      img_test_array.append(image)\n",
    "\n",
    "\n",
    "    return img_train_array, train_labels, img_test_array, test_labels\n",
    "\n",
    "\n",
    "#load data set\n",
    "x_train, y_train, x_test, y_test =load_dataset(data_dir,source_dir)\n",
    "x_test=    np.array(x_test)\n",
    "x_test.reshape(-1, IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "y_test= np.array(y_test)\n",
    "x_train = np.array(x_train)\n",
    "x_train.reshape(-1, IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "y_test = keras.utils.to_categorical(y_test,len(emotion_class))\n",
    "y_train = keras.utils.to_categorical(y_train,len(emotion_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
