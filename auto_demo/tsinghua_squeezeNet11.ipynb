{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c1c97d-fbfd-4e72-b196-4eb4af053026",
   "metadata": {},
   "source": [
    "# SqueezeNet 11 Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6672bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-09 14:15:40.073062: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.5.0\n",
      "Eager mode:  True\n",
      "WARNING:tensorflow:From /tmp/ipykernel_623475/2193406056.py:31: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-09 14:15:41.179827: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-09 14:15:41.181442: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-09-09 14:15:41.240339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:15:41.240892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 40 deviceMemorySize: 7.76GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2021-09-09 14:15:41.240918: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-09-09 14:15:41.244721: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-09-09 14:15:41.244767: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-09-09 14:15:41.246410: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-09-09 14:15:41.246648: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-09-09 14:15:41.247115: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-09-09 14:15:41.247859: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-09-09 14:15:41.247985: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-09-09 14:15:41.248073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:15:41.248573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:15:41.249011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-09-09 14:15:41.249037: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-09-09 14:15:41.559757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-09-09 14:15:41.559778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-09-09 14:15:41.559782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-09-09 14:15:41.559951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:15:41.560289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:15:41.560606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:15:41.560889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:0 with 6643 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import itertools\n",
    "import glob, os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from keras_tuner import HyperParameters\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner import RandomSearch,BayesianOptimization, Hyperband\n",
    "from tensorflow import keras\n",
    "\n",
    "import neptunecontrib.monitoring.kerastuner as npt_utils\n",
    "from keras_tuner import HyperParameters, Objective\n",
    "\n",
    "from keras_tuner.engine import hypermodel\n",
    "\n",
    "from model_all import SqueezeNetModel, SqueezeNet11Model,SqueezeNetSEAutoModel\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\n",
    "\n",
    "\n",
    "\n",
    "source_dir = '../TsinghuaFEDImages'\n",
    "data_dir = '../TsinghuaFED_class' # landmarks of each subject_emotion\n",
    "\n",
    "emotion_class = ['fear','neutral','surprise','anger','content','disgust','sad','happy']\n",
    "\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "nb_classes = len(emotion_class)\n",
    "ratio = 1.0\n",
    "EPOCH = 200\n",
    "\n",
    "def load_dataset(data_folder, img_dir):\n",
    "   \n",
    "\n",
    "  img_list = data_folder+'/train.csv'\n",
    "\n",
    "  f = open(img_list)\n",
    "  text = f.read()\n",
    "  lines = text.split()\n",
    "  train_img_files = []\n",
    "  train_labels = []\n",
    "  for l in lines:\n",
    "    data = l.split(',')\n",
    "    train_img_files.append(img_dir+'/'+data[0])\n",
    "    train_labels.append(int(data[1]))\n",
    "\n",
    "\n",
    "  img_train_array=[]\n",
    "  \n",
    "  img_train_array = []\n",
    "  for filename,label in zip(train_img_files,train_labels):\n",
    "      \n",
    "    image= cv2.imread( filename, cv2.COLOR_BGR2RGB)\n",
    "    image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "    image=np.array(image)\n",
    "    image = image.astype('float32')\n",
    "    image /= 255 \n",
    "    img_train_array.append(image)\n",
    "      \n",
    "\n",
    "  img_list = data_folder+'/test.csv'\n",
    "\n",
    "  f = open(img_list)\n",
    "  text = f.read()\n",
    "  lines = text.split()\n",
    "  test_img_files = []\n",
    "  test_labels = []\n",
    "  for l in lines:\n",
    "    data = l.split(',')\n",
    "    test_img_files.append(img_dir+'/'+data[0])\n",
    "    test_labels.append(int(data[1]))\n",
    "\n",
    "\n",
    "  img_test_array=[]\n",
    "\n",
    "  \n",
    "  for filename,label in zip(test_img_files,test_labels):\n",
    "\n",
    "    image= cv2.imread( filename, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "    image=np.array(image)\n",
    "    image = image.astype('float32')\n",
    "    image /= 255 \n",
    "    img_test_array.append(image)\n",
    "\n",
    "\n",
    "  return img_train_array, train_labels, img_test_array, test_labels\n",
    "    \n",
    "def transform_dataset(x_train,y_train,x_test,y_test):\n",
    "\n",
    "    \n",
    "    \n",
    "  x_test=    np.array(x_test)\n",
    "  x_test.reshape(-1, IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "  y_test= np.array(y_test)\n",
    "  x_train = np.array(x_train)\n",
    "  x_train.reshape(-1, IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "\n",
    "  y_train = np.array(y_train)\n",
    "  x_train = np.expand_dims(x_train, -1)\n",
    "  x_test = np.expand_dims(x_test, -1)\n",
    "  y_test = keras.utils.to_categorical(y_test,len(emotion_class))\n",
    "  y_train = keras.utils.to_categorical(y_train,len(emotion_class))\n",
    "  return x_train,y_train,x_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5beab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "neptune.init(project_qualified_name=' ',api_token='...') your credentials\n",
    "  #load data set\n",
    "x_train, y_train, x_test, y_test =load_dataset(data_dir,source_dir)\n",
    "\n",
    "x_train, y_train, x_test, y_test = transform_dataset(x_train, y_train, x_test, y_test )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748aec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/cchantra/keras-tuner/e/KER-99\n",
      "Search space summary\n",
      "Default search space size: 5\n",
      "learning_rate (Choice)\n",
      "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.0001], 'ordered': True}\n",
      "use_bypass (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "compression (Fixed)\n",
      "{'conditions': [], 'value': 1.0}\n",
      "dropout_rate (Choice)\n",
      "{'default': 0.1, 'conditions': [], 'values': [0.1, 0.5, 0.8], 'ordered': True}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'sgd'], 'ordered': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-09 14:16:30.792035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:16:30.792381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 40 deviceMemorySize: 7.76GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2021-09-09 14:16:30.792454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:16:30.792771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:16:30.793052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-09-09 14:16:30.793214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:16:30.793499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 40 deviceMemorySize: 7.76GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2021-09-09 14:16:30.793537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:16:30.793842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:16:30.794120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-09-09 14:16:30.794142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-09-09 14:16:30.794146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-09-09 14:16:30.794148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-09-09 14:16:30.794208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:16:30.794519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-09 14:16:30.794810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6643 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Squeeze random\n",
    "hp = HyperParameters()\n",
    "\n",
    "import neptune\n",
    "\n",
    "\n",
    "PROJECT = \"squeeze11AutoRandom_Tsinghua\"+str(ratio)\n",
    "neptune.create_experiment(PROJECT)\n",
    "\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNet11Model (classes=nb_classes, IMG_HEIGHT=IMG_HEIGHT,IMG_WIDTH=IMG_WIDTH)\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "  #build_SqueezeNet_11_fixed,\n",
    "  #build_squeezenet_auto_model,\n",
    "  #build_model,\n",
    "\n",
    "  hypermodel=mymodel,\n",
    "  hyperparameters = hp,\n",
    "  objective=Objective(\"val_accuracy\", direction=\"max\"),\n",
    "  max_trials=200,\n",
    "  executions_per_trial=1,\n",
    "  overwrite=True,\n",
    "  directory=\"mytest_tsinghua_dir\",\n",
    "  project_name=PROJECT,\n",
    "  #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "  logger = npt_utils.NeptuneLogger(),\n",
    "\n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c1c44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2924ab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 Complete [00h 00m 05s]\n",
      "val_accuracy: 0.07999999821186066\n",
      "\n",
      "Best val_accuracy So Far: 0.1257142871618271\n",
      "Total elapsed time: 00h 01m 45s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in mytest_tsinghua_dir/squeeze11AutoRandom_Tsinghua1.0\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: adam\n",
      "Score: 0.1257142871618271\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "Score: 0.11999999731779099\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: adam\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: adam\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: sgd\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: adam\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: sgd\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: adam\n",
      "Score: 0.07999999821186066\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                  max_lr=1e-3,\n",
    "                  steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                  epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "  tf.keras.callbacks.EarlyStopping('val_accuracy', patience=5),\n",
    "  #lr_finder,\n",
    "  #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "  #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models(num_models=10)\n",
    "\n",
    "\n",
    "\n",
    "tuner.results_summary()\n",
    "npt_utils.log_tuner_info(tuner)\n",
    "\n",
    "\n",
    "#best_model = tuner.get_best_models(num_models=1)[0]\n",
    "#best_model.evaluate(x_test, y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1b74ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  0 size 726600\n",
      "model  1 size 726600\n",
      "model  2 size 726600\n",
      "model  3 size 726600\n",
      "model  4 size 726600\n",
      "model  5 size 726600\n",
      "model  6 size 726600\n",
      "model  7 size 726600\n",
      "model  8 size 726600\n",
      "model  9 size 726600\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(models):\n",
    "    print('model  %d size %d'%(i,hypermodel.maybe_compute_model_size(models[i])))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e89027b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "924fd7bc",
   "metadata": {},
   "source": [
    "# Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a016ee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/cchantra/keras-tuner/e/KER-100\n",
      "Search space summary\n",
      "Default search space size: 5\n",
      "learning_rate (Choice)\n",
      "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.0001], 'ordered': True}\n",
      "use_bypass (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "compression (Fixed)\n",
      "{'conditions': [], 'value': 1.0}\n",
      "dropout_rate (Choice)\n",
      "{'default': 0.1, 'conditions': [], 'values': [0.1, 0.5, 0.8], 'ordered': True}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'sgd'], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    "#Squeeze Bayesian\n",
    "hp = HyperParameters()\n",
    "\n",
    "import neptune\n",
    "\n",
    "PROJECT = \"squeeze11Bayesian_Tsinghua\"+str(ratio)\n",
    "neptune.create_experiment(PROJECT)\n",
    "\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNet11Model  (classes=nb_classes,IMG_HEIGHT=IMG_HEIGHT,IMG_WIDTH=IMG_WIDTH)\n",
    "\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "  #build_SqueezeNet_11_fixed,\n",
    "  #build_squeezenet_auto_model,\n",
    "  #build_model,\n",
    "\n",
    "  hypermodel=mymodel,\n",
    "  hyperparameters = hp,\n",
    "  objective=\"val_accuracy\",\n",
    "  max_trials=100,\n",
    "  executions_per_trial=1,\n",
    "  overwrite=True,\n",
    "  directory=\"mytest_tsinghua_dir\",\n",
    "  project_name= PROJECT,\n",
    "  #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "  num_initial_points=2,\n",
    "  alpha=0.0001,\n",
    "  beta=2.6,\n",
    "  seed=None,\n",
    "  tune_new_entries=True,\n",
    "  allow_new_entries=True,\n",
    "  logger = npt_utils.NeptuneLogger(),\n",
    "\n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a53a08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 00m 09s]\n",
      "val_accuracy: 0.07999999821186066\n",
      "\n",
      "Best val_accuracy So Far: 0.1371428519487381\n",
      "Total elapsed time: 00h 17m 08s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Results summary\n",
      "Results in mytest_tsinghua_dir/squeeze11Bayesian_Tsinghua1.0\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "Score: 0.1371428519487381\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "Score: 0.1371428519487381\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "Score: 0.1314285695552826\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "Score: 0.1314285695552826\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "Score: 0.1314285695552826\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "Score: 0.1257142871618271\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "Score: 0.1257142871618271\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: adam\n",
      "Score: 0.1257142871618271\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "Score: 0.1257142871618271\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: adam\n",
      "Score: 0.1257142871618271\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                  max_lr=1e-3,\n",
    "                  steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                  epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "  tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "  #lr_finder,\n",
    "  #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "  #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models() #(num_models=10)\n",
    "tuner.results_summary()\n",
    "npt_utils.log_tuner_info(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94058e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  0 size 726600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "i = 0\n",
    "while i < len(models):\n",
    "    print('model  %d size %d'%(i,hypermodel.maybe_compute_model_size(models[i])))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "538d067e",
   "metadata": {},
   "source": [
    "# HyperBand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef50530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/cchantra/keras-tuner/e/KER-101\n",
      "Search space summary\n",
      "Default search space size: 5\n",
      "learning_rate (Choice)\n",
      "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.0001], 'ordered': True}\n",
      "use_bypass (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "compression (Fixed)\n",
      "{'conditions': [], 'value': 1.0}\n",
      "dropout_rate (Choice)\n",
      "{'default': 0.1, 'conditions': [], 'values': [0.1, 0.5, 0.8], 'ordered': True}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'sgd'], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#SqueezeAuto Hyper\n",
    "hp   = HyperParameters()\n",
    "\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[ 1e-3, 1e-4])\n",
    "\n",
    "\n",
    "import neptune\n",
    "\n",
    "PROJECT = \"squeeze11HyperTsinghua\"+str(ratio)\n",
    "neptune.create_experiment(PROJECT)\n",
    "\n",
    "\n",
    "#fn_name = globals()[\"build_SqueezeNet_11_fixed\"](hp)\n",
    "# SqueezeNet \n",
    "\n",
    "mymodel = SqueezeNet11Model(classes=nb_classes,IMG_HEIGHT=IMG_HEIGHT,IMG_WIDTH=IMG_WIDTH)\n",
    "\n",
    "\n",
    "\n",
    "tuner = Hyperband(\n",
    "  #build_SqueezeNet_11_fixed,\n",
    "  #build_squeezenet_auto_model,\n",
    "  #build_model,\n",
    "\n",
    "  hypermodel=mymodel,\n",
    "  hyperparameters = hp,\n",
    "  objective=\"val_accuracy\",\n",
    "  #max_trials=20,\n",
    "  executions_per_trial=1,\n",
    "  overwrite=True,\n",
    "  directory=\"mytest_tsinghua_dir\",\n",
    "  project_name=PROJECT,\n",
    "  #distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "  max_epochs=EPOCH,\n",
    "  logger = npt_utils.NeptuneLogger(),\n",
    "\n",
    ")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a200f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22 Complete [00h 00m 03s]\n",
      "val_accuracy: 0.07999999821186066\n",
      "\n",
      "Best val_accuracy So Far: 0.1314285695552826\n",
      "Total elapsed time: 00h 01m 17s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Results summary\n",
      "Results in mytest_tsinghua_dir/squeeze11HyperTsinghua1.0\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: adam\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.1314285695552826\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: adam\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.1257142871618271\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: adam\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.1257142871618271\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.11999999731779099\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: adam\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.8\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: True\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.1\n",
      "optimizer: sgd\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.07999999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "use_bypass: False\n",
      "compression: 1.0\n",
      "dropout_rate: 0.5\n",
      "optimizer: adam\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.07999999821186066\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"lr_finder = LRFinder(min_lr=1e-5, \n",
    "                  max_lr=1e-3,\n",
    "                  steps_per_epoch=np.ceil(EPOCH/batch_size),\n",
    "                  epochs=3)\"\"\"\n",
    "my_callbacks = [\n",
    "  tf.keras.callbacks.EarlyStopping('val_accuracy', patience=10),\n",
    "  #lr_finder,\n",
    "  #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "  #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "tuner.search(x_train[:int(ratio*len(x_train))], y_train[:int(ratio*len(y_train))], epochs=EPOCH, callbacks = my_callbacks,validation_data=(x_test[:int(ratio*len(x_test))], y_test[:int(ratio*len(y_test))]))\n",
    "\n",
    "models = tuner.get_best_models(num_models=10)\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54835c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  0 size 726600\n",
      "model  1 size 726600\n",
      "model  2 size 726600\n",
      "model  3 size 726600\n",
      "model  4 size 726600\n",
      "model  5 size 726600\n",
      "model  6 size 726600\n",
      "model  7 size 726600\n",
      "model  8 size 726600\n",
      "model  9 size 726600\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(models):\n",
    "    print('model  %d size %d'%(i,hypermodel.maybe_compute_model_size(models[i])))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647af2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
